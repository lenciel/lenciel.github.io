---
layout: post
comments: true
description: "上周去移动讲了讲人工智能。很多朋友说把材料分享一下，那就整理整理..."
title: "人工智能：成果、问题和展望（1）"
date: 2024-01-16 23:29:31 +0800
categories:

- AI
- talks
- rants

---

上周去移动[讲了讲人工智能](https://mp.weixin.qq.com/s/bRwXJWZ9yeyv18EKfnI6JA){:target="_blank"}。很多朋友说把材料分享一下，那就整理整理。

拿到的题目比较大，但出去讲或者听人家讲，主要是个照镜子的过程。稍微有点经验就知道，任何的知识、系统、手法，再具体再性感，都不是照搬过来就能解决自己的问题的。所以整个过程里面可能有那么一两句话，一两个场景能够给彼此启发，就应该知足了。

### 人工智能的成果

{% picture /downloads/images/2024_01/Slide11.png --alt Slide11.png %}\
<small>图 1. 治学先治史</small>

中国有句话叫治学先治史，我们搞清楚了过去，就搞清楚了未来。

如果我们去搜索人工智能历史，通常会看到的一个图，所谓的有两次寒冬，然后整体趋势是向上的。我会觉得，这其实就是 Roy Amara 的「阿拉玛定律」的一个体现：一个新技术出现，总是有短期被高估，长期被低估的现象。

Roy Amara 是[未来研究院](https://www.iftf.org/){:target="_blank"}的总裁，见得多了，所以得出了这个结论。

一个典型的例子是 GPS，它出现的时候，人们脑补了很多使用场景，大大地高估了。但实际上 GPS 铺开，是导航上的使用。今天的小朋友已经么有办法想象，车开到二环或者三环，一堆举着「带路」牌子的人在那里了。但是 GPS 通过导航普及，一旦上行、下行、端侧的基础设施建设完毕，我们看到，就出现了第二批的互联网公司：美团、滴滴、货车帮...因为这个时候，供需双方的地理信息可以准确拿到了。所以，GPS 短期被高估，但长期看，它仍然从根本上改变了很多的行业，以及我们的生活方式。

[阿拉玛定律](https://thevirtulab.com/what-is-amaras-law/){:target="_blank"}在人工智能领域会放大，反反复复地出现高峰低谷，我个人觉得是因为人工智能这项技术里面有「智能」这两个字。因为在普通人的认知里，和在学术界工业界这些真正的人工智能从业者认知里，「人工智能」这四个字根本就是两个意思。

接下来我就把它们拆开聊聊。

{% picture /downloads/images/2024_01/Slide12.png --alt Slide12.png %}\
<small>图 2. 普通人的人工智能</small>

我们如果去搜索人工智能，看到的图片一般都是这样的：有一个洁白光滑的机器人，它可以像人类一样去思考，甚至像人类一样去行动。这其实是代表了大众心中的人工智能：它是比较单一且明确的，就是对标的科幻小说或电影里面那，接近于我们今天说的 AGI 的存在。

所以我们每次说人工智能出现了重大突破，大家就会去想，这个东西是不是要来了。

{% picture /downloads/images/2024_01/Slide13.png --alt Slide13.png %}\
<small>图 3. 麦卡锡娇嗔背后的「模式」</small>

所以人工智能奠基人之一麦卡锡有一个娇嗔，说，一旦这东西工作了，大家就不把它叫人工智能了。

这话啥意思呢，就是人工智能它研究的各个领域，最终会变成算法，甚至算法最终稳定了之后，会定制成为硬件。比如我们大家现在付钱，或者刷门禁，一刷脸搞定了，大家觉得，这叫人脸识别。在 AlphaGo 出来的时候，我就听一些同学说我们的 AI 四小龙，号称人工智能，不就是做人脸识别吗。为什么？因为大家脑子里面对标的人工智能是类似于《her》里面的萨曼莎，所以就会觉得不够厉害。实际上，人脸识别也是一个可以做得很深的人工智能的领域。

所以我们可以看到人工智能历史上的一个模式：

1. 研究（包括技术）取得进展；
2. 人们开始激动并对 AI 抱着很高的期望；
3. 产业界开始加大投入并开发各种应用；
4. 应用未能满足期望，人工智能行业进入低谷；
5. 整个过程中，「某林频谱仪」常伴左右；

今天，因为人工智能大量以 chatbot 或者 agent 的形式存在，会增加用户的错误期望。

那么究竟什么是人工智能呢？比如电饭煲里面加一点 PID 控制是不是人工智能呢？

{% picture /downloads/images/2024_01/Slide14.png --alt Slide14.png %}\
<small>图 4. 学术界的「人工智能」</small>

在学术界和工程界或者说真正的 AI 从业者内部，AI 的定义不是那么明确的，而是相当模糊的。很多领域，比如自然语言处理，数字图像处理，增强学习，机器学习等等都被纳入其中。

这是因为比较公认的「AI」的提出，在 1956 年达特茅斯会议，整个命题就很宽泛。

大家可以看这张 slide，左边是这次头脑风暴形式的 workshop 讨论的主题，右边是主要的参与者，可以看到两个现象：首先，今天我们人工智能讨论的很多话题，比如神经网络，那个时候其实已经在讨论了；更重要是，讨论的话题以及参与者的背景，非常宽泛，横跨了计算机、自动化、逻辑、数学、心理学等多个学科。

比如，整个神经网络的提出主要就是维纳的学生，麦卡洛克（W. McCulloch）。图片里另外一位，塞弗里奇（O. Selfridge），名声没有那些拿了图灵奖的大，但其实是公认的模式识别之父，也做过一段时间明斯基的主管。他在 MIT 时一直和麦卡洛克一起在维纳手下工作。维纳对他非常欣赏，《控制论》的第一个读者就是他，但因为没有写博士论文，所以没有拿到博士学位：这大概跟他出身有关，日不落帝国牛津街上的 Selfridges 是他们家的。

{% picture /downloads/images/2024_01/Slide15.png --alt Slide15.png %}\
<small>图 5. 两种主义的持续斗争</small>

达特茅斯会议后，人工智能的研究蓬勃发展，但却分裂成了以「符号主义」和「连接主义」为主的数个阵营：

- 「符号主义（Symbolicism）」，又称逻辑主义，主张用符号、公理和逻辑体系搭建一套人工智能系统；
- 「连接主义（Connectionism）」，又叫仿生学派，主张模仿人类的神经元，用神经网络的连接机制实现人工智能；

「符号主义」里的「符号」，就是表示客观事物或者事物间关系的「字符串」。通过它们对人类的「认知」进行编码，形成可逻辑演算的系统，是「符号主义」的核心思想。

因此，「符号主义」的核心特点在于：

- 推理的过程是可解释的；
- 出现错误，是可以追溯和定位的；
- 符号化方法对于知识的表达能力强，能够应对较复杂的知识推理；

这些优势让「符号主义」在 AI 领域长期一枝独秀，被广泛地运用于「自动定理证明」和「专家系统」的构建。尤其是「专家系统」的成功开发与应用，为人工智能走向工程应用做出了杰出贡献。

但「符号主义」也有一些根深蒂固的困境。比如，[哥德尔不完全性定理](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems){:target="_blank"}（证明了对于任何一个公理化系统，都存在此公理体系所无法判定真伪的命题），就让使用「符号主义」构建的系统面临泛化的问题。再比如，既然要用符号来表达事物和事物之间的关系，这里就有一个谁可以把客观世界抽象成这些符号，并解释给其他人的问题。

因为这些没法解决的问题，「人工智能」曾经有很长一段时间发展得并不算好。我读书的时候，国内的人工智能集中在自动化专业。当时我感觉自动化专业主要有三拨人：

- 搞控制理论的，主要是用数学工具在矩阵推导；
- 搞应用系统的，主要是想要做出国产的工控机；
- 搞算法的，主要是遗传算法、神经网络；

前面两帮人是主流，彼此不太看得上，但是他们一起看不上搞算法的。到了今天，人工智能在很多学校已经和计算机、自动化平起平坐，有自己的学院了。

这似乎全靠「神经网络」的大获全胜。实际上，今天我们基本上听不到「连接主义」，只听得到这个门派的代表作：「神经网络」，即通过模仿人类的神经元组网，实现人工智能。

需要注意的是，它说的神经元是 1943 年生理学家麦卡洛克(McCulloch)和数理逻辑学家皮茨(Pitts)创立的脑模型，即 [MP](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1){:target="_blank"} 模型，跟今天我们对脑神经的认知有很大的不同。但报导 ChatGPT 的媒体好像很喜欢配一张现代科学理解下的大脑透视图，显得这套东西多么先进甚至科幻。

实际上，神经网络提出快 90 年了，并且曾多次被主流圈子抛弃。比如右边这张图上，大家可以看到 Marvin Minsky，虽然他 1954 年的博士论文你看标题就知道很「[连接主义](https://searchworks.stanford.edu/view/970633){:target="_blank"}」，但后来却改旗易帜，成了「符号主义」领袖。特别是 1969 年，作为「符号主义」的领军人物，Marvin Minsky 和 ​​Seymour Papert 出版了一本书《[Perceptrons](https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational){:target="_blank"}》，对连 XOR 运算都不会的神经网络判了死刑。

但大家看到右边这张图就知道，神经网络后来又再次突飞猛进，成为主流，这是为什么呢？

{% picture /downloads/images/2024_01/Slide16.png --alt Slide16.png %}\
<small>图 6. 从「玩具」到「产品」</small>

我个人觉得，神经网络再次成为主流，主要是下面三方面的进展综合造成的：

- 算力
- 数据
- 算法

算力方面，今天很多人都对处境艰辛的 Hinton 和同样处境艰辛的黄教主，是怎么把 GPU 用起来的故事耳熟能详了。

互联网的兴起，也带来了大量公开的数据，让大规模神经网络的训练成为可能。

当然，算法也有不断的进步，这里列出了几个里程碑式的算法进展，最后一个就是 transformer。

有了这三方面的加持，人工智能在很多细分领域的能力超越了人类，从「玩具」正式变成具备一定生产力的「产品」。

{% picture /downloads/images/2024_01/Slide17.png --alt Slide17.png %}\
<small>图 7. 从「炼丹」到「系统」</small>

人工智能在过去一两年的发展，还有一个特点，就是从「炼丹」到「系统」。

什么意思？我们判断一个行业或者叫赛道是不是进入了收敛期，主要看是不是出现了分工和分层。

比如倒过去五六年，如果你问一个公司的技术负责人在干嘛，Ta 大概率会说，在做「微服务」或者「中台」。

这就有点像你问晚年的牛顿在干嘛，他会说，他在炼金。

炼金距离真正的化学，主要就是差一张门捷列夫的化学元素周期表。

类似的，一旦有了分工和分层，一个体系就建立了。比如你最近两三年问技术负责人，Ta 会说，在做微服务里面的哪个模块，或者说哪种中台，这就是收敛的迹象。

目前，人工智能也已经有了明确的分工和分层。有干基础设施的，有干应用层的。干应用的，还会细分，有干纵向的，有干垂类的。

{% picture /downloads/images/2024_01/Slide18.png --alt Slide18.png %}\
<small>图 8. 还是有争议</small>

但是尽管有了长足进步，参加各种的人工智能讨论和会议，会有一个明显的感受，就是一些核心问题还没有收敛，比如这里列出的这些。

并且有个特点，不仅仅是普通从业者对这些问题有争议，行业领军人物也不例外。IEEE Spectrum 为此专门搞了个[计分卡](https://spectrum.ieee.org/artificial-general-intelligence){:target="_blank"}，把 AI 领域的 22 位杰出领袖的意见做了一个整理：可以看到，以 Rodney Brooks、Yann Lecun 为代表的大部分人，还是觉得哪怕 GPT-4 也没有任何发展为 AGI 的可能。少数派主要是忧心忡忡的 Hinton 和 Sam Altman。

为什么能有这么大争议呢？

{% picture /downloads/images/2024_01/Slide19.png --alt Slide19.png %}\
<small>图 9. 为什么还有很多争议</small>

我对这个问题的看法是，因为我们讨论的过程中，涉及很多「suitcase words」，所以根本没法聊清楚。

「suitcase words」是 Minsky 在 2006 年的这本《Emotional Machine》里提出的概念，就是说我们讨论人工智能的时候使用的很多词是，类似于出行的时候用手提箱打包一样，一个抽象了打包好的概念，外延和边界都非常模糊。

下面举两个例子。

{% picture /downloads/images/2024_01/Slide20.png --alt Slide20.png %}\
<small>图 10. 例子一：机器学习</small>

比如「学习」这个词。

Arthur Samuel 使用「机器学习」这个词，我们看原文，其实是一个类比。

而且，机器学习几个主要分支，监督学习、无监督学习、深度学习等等，都是已经提出了大几十年的概念。

它和人的「学习」有很大不同。

人学东西有两个显著特点，即「抽象」和「泛化」能力。

GPT-3 的训练使用了数千亿个 token，与此相对，普通儿童听到的所有单词，不排重还不到一个亿。

人类不需要那么大的数据量做训练，因为人擅长各种层次的抽象，并能够把抽象出来的认知泛化。

比如，你买一把香蕉告诉四五岁的小孩儿这是香蕉，从此以后不管是长的短的，熟的生的，黄的绿的甚至闷烂了发黑的香蕉摆在 Ta 面前， Ta 都知道这是香蕉。为什么说机器「学习」和人的「学习」完全不是一回事？因为你绝不会感到教小朋友某个东西，是在调 Ta 的参数。

{% picture /downloads/images/2024_01/Slide21.png --alt Slide21.png %}\
<small>图 11. 例子二：中文屋问题</small>

再比如一个经典的哲学问题：「[中文屋](https://cloud.tencent.com/developer/news/365176){:target="_blank"}」。

如果简单地思考，肯定觉得如果屋子里面这个人完全不懂中文，他没有中文能力。

但是我们如果多想几步，比如这个人经过相当长时间，已经把各种中文的 rule book 内化了，外面给他的中文问题，他可以在头脑里面查找给出回复，那么他会不会中文？

这个时候，他其实已经可以脱离那间屋子了，非常接近于一个经过训练的「大模型」，对不对？

实际上「中文屋」是一个非常深的问题，可以聊很多，比如什么是语法和语义，什么是意识和心智，限于时间，我们制定这些东西都是「suitcase words」即可。

{% picture /downloads/images/2024_01/Slide22.png --alt Slide22.png %}\
<small>图 12. 人工智能的成果总结</small>

总结一下第一部分，人工智能的成果：

1. 大部分人心中的 AI 和从业者心中的 AI 很不一样，所以目前的这些成果是否会再次遇到「短期被高估」的问题，还有待时间检验；
2. 得益于数据、算力、软硬件框架等各方面的进展和成熟，以及类似于 Transformer 这样算法上的创新，神经网络在人工智能的各个领域都取得了一骑绝尘的巨大进展，很多细分领域的能力超越了人类，有了从「玩具」到「产品」，从「炼丹」到「系统」的趋势；
3. 但构建当今 AI 系统的大部分理论仍然没有大的变化，因此 Suitcase Words 这样的问题会长期存在，很多类似于「有没有涌现」，「会不会通向 AGI」的问题很难讨论出结果；

